La entropía de Shannon es una de las métricas más importantes en la teoría de la información. La entropía mide la incertidumbre asociada a una variable aleatoria, es decir, el valor esperado de la información contenida en el mensaje (en informática clásica se mide en bits).

El concepto fue introducido por Claude E. Shannon en el artículo “ Una teoría matemática de la comunicación ” (1948). La entropía de Shannon permite estimar el número mínimo promedio de bits necesarios para codificar una cadena de símbolos en función del tamaño del alfabeto y la frecuencia de los símbolos.

A continuación encontrará una calculadora sencilla que le ayudará a comprender el concepto.

Pegue su cadena (por ejemplo, "1100101", "Lorem ipsum", "abracadabra") para calcular la entropía de Shannon

![Captura de pantalla (492)](https://github.com/user-attachments/assets/510ff737-7dda-4394-9515-438ff0595a35)

Como se calcula ?

Sea un mensaje "X" que está escrito en un alfabeto de "n" simbolos (letras)

Sea "p(xi)" la frecuencia ó probabilidad del símbolo "xi" en el mensaje "X"
(# apariciones de xi / longitud del mensaje)

![Captura de pantalla2 (493)](https://github.com/user-attachments/assets/8e463dc9-6260-4b6d-b47f-b6661a9dd814)

En este otro caso vemos que el mínimo número de bits por símbolo necesarios en promedio para codificar el mensaje es "4,456"

![Captura de pantalla (494)](https://github.com/user-attachments/assets/19776e24-49d3-4007-86ac-2443358899a1)
